{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf35a0c8-4cab-4c91-bc62-5c190b6b1e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e10d702-0e8d-43a7-877b-048633d92be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset Class for Loading Images\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, dataset_path, image_size=(128, 128)):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.image_size = image_size\n",
    "        self.images = []\n",
    "        \n",
    "        # Load all images\n",
    "        for filename in sorted(os.listdir(dataset_path)):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                img = Image.open(os.path.join(dataset_path, filename)).resize(image_size)\n",
    "                img = np.array(img).astype(np.float32) / 255.0\n",
    "                self.images.append(img)\n",
    "        \n",
    "        self.images = torch.tensor(np.transpose(self.images, (0, 3, 1, 2)))  # N, C, H, W\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx]\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"dataset/\"  # Adjust this to your dataset path\n",
    "dataset = ImageDataset(dataset_path)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Define Autoencoder Model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1), nn.ReLU(),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, latent_dim)  # 16x16 comes from downsampling\n",
    "\n",
    "        self.fc2 = nn.Linear(latent_dim, 128 * 16 * 16)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 3, stride=2, padding=1, output_padding=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc1(x)  # Latent space\n",
    "        x = self.fc2(x)  # Expand back\n",
    "        x = x.view(x.size(0), 128, 16, 16)  # Reshape for decoder\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Initialize the model and move it to the device (MPS or CPU)\n",
    "model = Autoencoder().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2041211-66ae-494a-8407-d6f5ec6ccbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0025\n",
      "Epoch [200/1000], Loss: 0.0014\n",
      "Epoch [300/1000], Loss: 0.0009\n",
      "Epoch [400/1000], Loss: 0.0005\n",
      "Epoch [500/1000], Loss: 0.0004\n",
      "Epoch [600/1000], Loss: 0.0003\n",
      "Epoch [700/1000], Loss: 0.0002\n",
      "Epoch [800/1000], Loss: 0.0002\n",
      "Epoch [900/1000], Loss: 0.0002\n",
      "Epoch [1000/1000], Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop (overfitting to the data)\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images in data_loader:\n",
    "        images = images.to(device)  # Move images to MPS/CPU\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, images)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9cd9aae-34fb-453c-8ee1-b9df4b6b4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an image\n",
    "def generate_image(model, idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = dataset[idx].unsqueeze(0).to(device)  # Get image and move to device\n",
    "        output = model(img)\n",
    "        output = output.squeeze(0).cpu().numpy().transpose(1, 2, 0) * 255\n",
    "        output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "        Image.fromarray(output).show()\n",
    "\n",
    "generate_image(model, 1)  # Test with the first image in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caa45a84-5711-4394-bb7e-df60d738dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'autoencoder_model.pth')\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03996dcb-d99a-4f19-9275-aa1d51426a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/54pvsn9x69qc9l6htqv8078r0000gn/T/ipykernel_69053/540588821.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('autoencoder_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# Load the model (after training or in a new session)\n",
    "model = Autoencoder().to(device)\n",
    "model.load_state_dict(torch.load('autoencoder_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94241f74-06a3-4870-a8b5-92f1a1335a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate an image from a saved model\n",
    "def generate_image(model, idx):\n",
    "    with torch.no_grad():\n",
    "        img = dataset[idx].unsqueeze(0).to(device)  # Get the image and move it to device\n",
    "        output = model(img)\n",
    "        output = output.squeeze(0).cpu().numpy().transpose(1, 2, 0) * 255\n",
    "        output = np.clip(output, 0, 255).astype(np.uint8)\n",
    "        Image.fromarray(output).show()\n",
    "\n",
    "# Generate an image using the loaded model\n",
    "generate_image(model, 0)  # Generate an image from the first example in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0bfcf-4e2e-4813-8207-699f620f4d12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
